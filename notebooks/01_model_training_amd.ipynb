{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant Doctor - Entra√Ænement du Mod√®le CNN\n",
    "\n",
    "**Configuration:** AMD RX 7900 XT avec TensorFlow-DirectML (Windows)\n",
    "\n",
    "Ce notebook entra√Æne un mod√®le EfficientNet-B0 pour la classification des maladies de plantes sur le dataset PlantVillage (38 classes).\n",
    "\n",
    "## Fonctionnalit√©s:\n",
    "- **Checkpoints automatiques** - Sauvegarde √† chaque epoch, reprise possible\n",
    "- **Graphiques en temps r√©el** - Visualisation pendant l'entra√Ænement\n",
    "- **D√©tection d'overfitting** - Alerte si val_loss diverge trop\n",
    "- **Estimation du temps** - Temps restant estim√©\n",
    "\n",
    "## Temps estim√©:\n",
    "- **Phase 1 (10 epochs):** ~30-45 minutes (GPU AMD)\n",
    "- **Phase 2 (15 epochs):** ~60-90 minutes (GPU AMD)\n",
    "- **Total:** ~1h30 √† 2h30 selon la configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances\n",
    "\n",
    "Pour AMD sur Windows, on utilise **tensorflow-directml** qui supporte les GPU AMD via DirectX 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.6' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/peric/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Installation tensorflow-directml pour AMD GPU sur Windows\n",
    "# Ex√©cuter UNE SEULE FOIS\n",
    "# !pip install tensorflow-directml-plugin tensorflow==2.10.0\n",
    "\n",
    "# Alternative: tensorflow standard (CPU) si DirectML pose probl√®me\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Autres d√©pendances\n",
    "# !pip install pillow matplotlib scikit-learn kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# V√©rifier si DirectML est utilis√©\n",
    "try:\n",
    "    devices = tf.config.list_physical_devices()\n",
    "    gpu_found = any('GPU' in str(d) or 'DML' in str(d) for d in devices)\n",
    "    print(f\"GPU/DirectML d√©tect√©: {'Oui' if gpu_found else 'Non (CPU mode)'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Mode CPU (DirectML non d√©tect√©): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T√©l√©chargement du Dataset PlantVillage\n",
    "\n",
    "Le dataset PlantVillage contient ~54,000 images de feuilles de plantes r√©parties en 38 classes (maladies + plantes saines).\n",
    "\n",
    "**Option 1:** T√©l√©charger depuis Kaggle (recommand√©)\n",
    "- Dataset: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
    "\n",
    "**Option 2:** T√©l√©charger manuellement et placer dans `data/PlantVillage/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "PROJECT_ROOT = Path(\"C:/TFE-4\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATASET_DIR = DATA_DIR / \"PlantVillage\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Cr√©er les dossiers si n√©cessaire\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Exists: {DATASET_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: T√©l√©charger via Kaggle API\n",
    "# N√©cessite un fichier kaggle.json dans ~/.kaggle/\n",
    "\n",
    "# !kaggle datasets download -d emmarex/plantdisease -p {DATA_DIR}\n",
    "# !unzip {DATA_DIR}/plantdisease.zip -d {DATA_DIR}\n",
    "\n",
    "# Option 2: T√©l√©chargement manuel\n",
    "# 1. Aller sur https://www.kaggle.com/datasets/emmarex/plantdisease\n",
    "# 2. T√©l√©charger et extraire dans C:/TFE-4/data/PlantVillage/\n",
    "# Structure attendue:\n",
    "#   data/PlantVillage/\n",
    "#     ‚îú‚îÄ‚îÄ Apple___Apple_scab/\n",
    "#     ‚îú‚îÄ‚îÄ Apple___Black_rot/\n",
    "#     ‚îú‚îÄ‚îÄ ...\n",
    "#     ‚îî‚îÄ‚îÄ Tomato___healthy/\n",
    "\n",
    "print(\"T√©l√©chargez le dataset PlantVillage et placez-le dans:\")\n",
    "print(str(DATASET_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le dataset\n",
    "if DATASET_DIR.exists():\n",
    "    classes = sorted([d.name for d in DATASET_DIR.iterdir() if d.is_dir()])\n",
    "    print(f\"Nombre de classes: {len(classes)}\")\n",
    "    print(f\"\\nClasses trouv√©es:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        count = len(list((DATASET_DIR / cls).glob('*')))\n",
    "        print(f\"  {i:2d}. {cls}: {count} images\")\n",
    "else:\n",
    "    print(\"Dataset non trouv√©! T√©l√©chargez-le d'abord.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres\n",
    "IMG_SIZE = 224  # EfficientNet-B0 input size\n",
    "BATCH_SIZE = 32  # R√©duire si manque de VRAM (16 ou 8)\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# Data augmentation pour l'entra√Ænement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Pas d'augmentation pour la validation\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les g√©n√©rateurs\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    DATASET_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Sauvegarder les labels\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nNombre de classes: {num_classes}\")\n",
    "print(f\"Images d'entra√Ænement: {train_generator.samples}\")\n",
    "print(f\"Images de validation: {val_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mapping des classes\n",
    "class_labels = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "# Cr√©er un fichier JSON avec les labels\n",
    "labels_file = DATA_DIR / \"class_labels.json\"\n",
    "with open(labels_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(class_labels, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Labels sauvegard√©s dans: {labels_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction du Mod√®le (EfficientNet-B0 + Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, fine_tune_at=100):\n",
    "    \"\"\"\n",
    "    Cr√©er un mod√®le EfficientNet-B0 avec transfer learning.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Nombre de classes de sortie\n",
    "        fine_tune_at: Couche √† partir de laquelle fine-tuner (0 = tout geler)\n",
    "    \"\"\"\n",
    "    # Charger EfficientNet-B0 pr√©-entra√Æn√© sur ImageNet\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Geler les couches de base\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Construire le mod√®le complet\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model, base_model = create_model(num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le mod√®le\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entra√Ænement - Phase 1 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CALLBACKS AVANC√âS AVEC GRAPHIQUES EN TEMPS R√âEL\n",
    "# ============================================================\n",
    "\n",
    "class TrainingMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personnalis√© pour:\n",
    "    - Afficher la progression avec barre\n",
    "    - Graphiques en temps r√©el\n",
    "    - D√©tection d'overfitting\n",
    "    - Estimation du temps restant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, total_epochs, phase_name=\"Training\"):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.phase_name = phase_name\n",
    "        self.history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        self.epoch_times = []\n",
    "        self.start_time = None\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {self.phase_name} - {self.total_epochs} epochs\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Enregistrer le temps\n",
    "        epoch_time = time.time() - self.epoch_start\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Enregistrer les m√©triques\n",
    "        self.history['loss'].append(logs.get('loss', 0))\n",
    "        self.history['accuracy'].append(logs.get('accuracy', 0))\n",
    "        self.history['val_loss'].append(logs.get('val_loss', 0))\n",
    "        self.history['val_accuracy'].append(logs.get('val_accuracy', 0))\n",
    "        \n",
    "        # Calculer le temps restant\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        remaining_epochs = self.total_epochs - (epoch + 1)\n",
    "        eta = timedelta(seconds=int(avg_epoch_time * remaining_epochs))\n",
    "        \n",
    "        # Barre de progression\n",
    "        progress = (epoch + 1) / self.total_epochs\n",
    "        bar_length = 30\n",
    "        filled = int(bar_length * progress)\n",
    "        bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
    "        \n",
    "        # D√©tecter l'overfitting\n",
    "        overfitting_warning = \"\"\n",
    "        if len(self.history['val_loss']) > 3:\n",
    "            recent_val_loss = self.history['val_loss'][-3:]\n",
    "            recent_train_loss = self.history['loss'][-3:]\n",
    "            gap = np.mean(recent_val_loss) - np.mean(recent_train_loss)\n",
    "            if gap > 0.3:\n",
    "                overfitting_warning = \" ‚ö†Ô∏è OVERFITTING POSSIBLE\"\n",
    "        \n",
    "        # Afficher la progression\n",
    "        clear_output(wait=True)\n",
    "        print(f\"\\n{self.phase_name}\")\n",
    "        print(f\"[{bar}] {epoch+1}/{self.total_epochs} ({progress*100:.0f}%)\")\n",
    "        print(f\"\")\n",
    "        print(f\"üìä Epoch {epoch+1} Results:\")\n",
    "        print(f\"   Loss:     {logs.get('loss', 0):.4f} (train) | {logs.get('val_loss', 0):.4f} (val)\")\n",
    "        print(f\"   Accuracy: {logs.get('accuracy', 0)*100:.2f}% (train) | {logs.get('val_accuracy', 0)*100:.2f}% (val)\")\n",
    "        print(f\"\")\n",
    "        print(f\"‚è±Ô∏è  Temps epoch: {timedelta(seconds=int(epoch_time))} | ETA: {eta}{overfitting_warning}\")\n",
    "        \n",
    "        # Afficher le graphique\n",
    "        self._plot_progress()\n",
    "        \n",
    "    def _plot_progress(self):\n",
    "        \"\"\"Afficher les graphiques de progression.\"\"\"\n",
    "        if len(self.history['loss']) < 1:\n",
    "            return\n",
    "            \n",
    "        epochs = range(1, len(self.history['loss']) + 1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0].plot(epochs, self.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "        axes[0].plot(epochs, self.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "        axes[0].fill_between(epochs, self.history['loss'], self.history['val_loss'], \n",
    "                             alpha=0.2, color='gray')\n",
    "        axes[0].set_title('Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[1].plot(epochs, [a*100 for a in self.history['accuracy']], 'b-', \n",
    "                     label='Train', linewidth=2)\n",
    "        axes[1].plot(epochs, [a*100 for a in self.history['val_accuracy']], 'r-', \n",
    "                     label='Validation', linewidth=2)\n",
    "        axes[1].set_title('Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy (%)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_ylim([0, 100])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {self.phase_name} termin√©!\")\n",
    "        print(f\"  Temps total: {timedelta(seconds=int(total_time))}\")\n",
    "        print(f\"  Meilleure val_accuracy: {max(self.history['val_accuracy'])*100:.2f}%\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# Callbacks standards\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODEL_DIR / 'checkpoint_phase1_epoch{epoch:02d}.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,  # Sauvegarder chaque epoch pour reprise\n",
    "    save_weights_only=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODEL_DIR / 'best_model_phase1.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Callbacks configur√©s:\")\n",
    "print(\"   - Sauvegarde checkpoint chaque epoch\")\n",
    "print(\"   - Sauvegarde meilleur mod√®le\")\n",
    "print(\"   - Early stopping (patience=5)\")\n",
    "print(\"   - R√©duction learning rate automatique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Entra√Æner uniquement les couches ajout√©es (base gel√©e)\n",
    "EPOCHS_PHASE1 = 10\n",
    "\n",
    "# Cr√©er le moniteur d'entra√Ænement\n",
    "monitor_phase1 = TrainingMonitor(EPOCHS_PHASE1, \"üå± Phase 1: Feature Extraction\")\n",
    "\n",
    "# Liste des callbacks\n",
    "callbacks_phase1 = [\n",
    "    monitor_phase1,\n",
    "    checkpoint_callback,\n",
    "    best_model_callback,\n",
    "    early_stopping,\n",
    "    reduce_lr\n",
    "]\n",
    "\n",
    "print(\"üöÄ D√©marrage Phase 1...\")\n",
    "print(f\"   Epochs: {EPOCHS_PHASE1}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Steps par epoch: {train_generator.samples // BATCH_SIZE}\")\n",
    "print(\"\")\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=0  # On utilise notre propre affichage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPRISE D'ENTRA√éNEMENT (si interrompu)\n",
    "# ============================================================\n",
    "# D√©commentez cette cellule si vous devez reprendre l'entra√Ænement\n",
    "\n",
    "\"\"\"\n",
    "# Trouver le dernier checkpoint\n",
    "import glob\n",
    "checkpoints = sorted(glob.glob(str(MODEL_DIR / 'checkpoint_phase1_epoch*.keras')))\n",
    "if checkpoints:\n",
    "    latest_checkpoint = checkpoints[-1]\n",
    "    print(f\"üîÑ Reprise depuis: {latest_checkpoint}\")\n",
    "    \n",
    "    # Extraire le num√©ro d'epoch\n",
    "    import re\n",
    "    match = re.search(r'epoch(\\d+)', latest_checkpoint)\n",
    "    start_epoch = int(match.group(1)) if match else 0\n",
    "    \n",
    "    # Charger le mod√®le\n",
    "    model = keras.models.load_model(latest_checkpoint)\n",
    "    print(f\"   Mod√®le charg√©, reprise √† l'epoch {start_epoch + 1}\")\n",
    "    \n",
    "    # Ajuster les epochs restants\n",
    "    EPOCHS_PHASE1 = 10\n",
    "    remaining_epochs = EPOCHS_PHASE1 - start_epoch\n",
    "    print(f\"   Epochs restants: {remaining_epochs}\")\n",
    "else:\n",
    "    print(\"Aucun checkpoint trouv√©, d√©marrage depuis le d√©but\")\n",
    "\"\"\"\n",
    "print(\"üí° Pour reprendre un entra√Ænement interrompu, d√©commentez la cellule ci-dessus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entra√Ænement - Phase 2 (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompiler avec un learning rate plus bas\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks pour phase 2\n",
    "checkpoint_phase2 = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODEL_DIR / 'checkpoint_phase2_epoch{epoch:02d}.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_model_phase2 = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(MODEL_DIR / 'best_model_phase2.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "early_stopping_phase2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_phase2 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tuning\n",
    "EPOCHS_PHASE2 = 15\n",
    "\n",
    "# Cr√©er le moniteur d'entra√Ænement\n",
    "monitor_phase2 = TrainingMonitor(EPOCHS_PHASE2, \"üåø Phase 2: Fine-Tuning\")\n",
    "\n",
    "# Liste des callbacks\n",
    "callbacks_phase2 = [\n",
    "    monitor_phase2,\n",
    "    checkpoint_phase2,\n",
    "    best_model_phase2,\n",
    "    early_stopping_phase2,\n",
    "    reduce_lr_phase2\n",
    "]\n",
    "\n",
    "print(\"üöÄ D√©marrage Phase 2 (Fine-Tuning)...\")\n",
    "print(f\"   Epochs: {EPOCHS_PHASE2}\")\n",
    "print(f\"   Learning rate: 1e-5 (r√©duit)\")\n",
    "print(\"\")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tuning\n",
    "EPOCHS_PHASE2 = 15\n",
    "\n",
    "print(\"\\nPhase 2: Fine-Tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_phase2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. √âvaluation et Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GRAPHIQUES FINAUX ET ANALYSE D'OVERFITTING\n",
    "# ============================================================\n",
    "\n",
    "def plot_final_history(history1, history2, save_path):\n",
    "    \"\"\"Graphique complet avec analyse d'overfitting.\"\"\"\n",
    "    \n",
    "    # Combiner les historiques\n",
    "    acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "    val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "    loss = history1.history['loss'] + history2.history['loss']\n",
    "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    phase1_end = len(history1.history['accuracy'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Loss\n",
    "    axes[0, 0].plot(epochs, loss, 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, val_loss, 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 0].axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='Fine-tuning start')\n",
    "    axes[0, 0].fill_between(epochs, loss, val_loss, alpha=0.2, color='orange')\n",
    "    axes[0, 0].set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy\n",
    "    axes[0, 1].plot(epochs, [a*100 for a in acc], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, [a*100 for a in val_acc], 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 1].axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='Fine-tuning start')\n",
    "    axes[0, 1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 100])\n",
    "    \n",
    "    # 3. Overfitting Gap (Train - Val)\n",
    "    gap_acc = [t - v for t, v in zip(acc, val_acc)]\n",
    "    gap_loss = [v - t for t, v in zip(loss, val_loss)]\n",
    "    \n",
    "    axes[1, 0].plot(epochs, [g*100 for g in gap_acc], 'purple', linewidth=2, label='Accuracy Gap')\n",
    "    axes[1, 0].axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    axes[1, 0].axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='Warning threshold (5%)')\n",
    "    axes[1, 0].axhline(y=10, color='red', linestyle='--', alpha=0.7, label='Danger threshold (10%)')\n",
    "    axes[1, 0].fill_between(epochs, 0, [g*100 for g in gap_acc], alpha=0.3, \n",
    "                            color=['green' if g < 0.05 else 'orange' if g < 0.1 else 'red' for g in gap_acc])\n",
    "    axes[1, 0].set_title('Overfitting Detection (Train - Val Accuracy)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Gap (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Learning Rate Effect\n",
    "    axes[1, 1].plot(epochs, val_acc, 'r-', linewidth=2, label='Val Accuracy')\n",
    "    axes[1, 1].axvline(x=phase1_end, color='green', linestyle='--', alpha=0.7, label='LR: 0.001 ‚Üí 0.00001')\n",
    "    \n",
    "    # Marquer le meilleur epoch\n",
    "    best_epoch = np.argmax(val_acc) + 1\n",
    "    best_val_acc = max(val_acc)\n",
    "    axes[1, 1].scatter([best_epoch], [best_val_acc], color='gold', s=200, zorder=5, \n",
    "                       marker='‚òÖ', label=f'Best: {best_val_acc*100:.2f}% (epoch {best_epoch})')\n",
    "    \n",
    "    axes[1, 1].set_title('Best Model Selection', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Rapport d'overfitting\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RAPPORT D'ANALYSE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_gap = gap_acc[-1] * 100\n",
    "    if final_gap < 5:\n",
    "        status = \"‚úÖ EXCELLENT\"\n",
    "        msg = \"Pas d'overfitting d√©tect√©\"\n",
    "    elif final_gap < 10:\n",
    "        status = \"‚ö†Ô∏è ATTENTION\"\n",
    "        msg = \"L√©ger overfitting, consid√©rez plus de r√©gularisation\"\n",
    "    else:\n",
    "        status = \"‚ùå OVERFITTING\"\n",
    "        msg = \"Overfitting significatif, utilisez early stopping ou plus de donn√©es\"\n",
    "    \n",
    "    print(f\"\\n{status}: {msg}\")\n",
    "    print(f\"   Gap final (train-val): {final_gap:.2f}%\")\n",
    "    print(f\"   Meilleure val_accuracy: {max(val_acc)*100:.2f}% (epoch {best_epoch})\")\n",
    "    print(f\"   Loss finale: {val_loss[-1]:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# G√©n√©rer les graphiques\n",
    "plot_final_history(history1, history2, MODEL_DIR / 'training_history_complete.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'historique d'entra√Ænement\n",
    "def plot_history(history1, history2):\n",
    "    # Combiner les historiques\n",
    "    acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "    val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "    loss = history1.history['loss'] + history2.history['loss']\n",
    "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    phase1_end = len(history1.history['accuracy'])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(epochs, acc, 'b-', label='Training')\n",
    "    ax1.plot(epochs, val_acc, 'r-', label='Validation')\n",
    "    ax1.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning start')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(epochs, loss, 'b-', label='Training')\n",
    "    ax2.plot(epochs, val_loss, 'r-', label='Validation')\n",
    "    ax2.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning start')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(MODEL_DIR / 'training_history.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history1, history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = MODEL_DIR / 'efficientnet_plant_disease.keras'\n",
    "model.save(final_model_path)\n",
    "print(f\"\\nMod√®le sauvegard√©: {final_model_path}\")\n",
    "\n",
    "# Aussi sauvegarder en format .h5 (compatibilit√©)\n",
    "h5_model_path = MODEL_DIR / 'efficientnet_plant_disease.h5'\n",
    "model.save(h5_model_path)\n",
    "print(f\"Mod√®le H5 sauvegard√©: {h5_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(model, img_path, class_labels):\n",
    "    \"\"\"\n",
    "    Pr√©dire la classe d'une image.\n",
    "    \"\"\"\n",
    "    # Charger et pr√©traiter l'image\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Pr√©diction\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    # R√©cup√©rer le nom de la classe\n",
    "    class_name = class_labels[str(predicted_class)]\n",
    "    \n",
    "    return class_name, confidence, predictions[0]\n",
    "\n",
    "# Test avec une image du dataset\n",
    "# test_img = list(DATASET_DIR.glob('*/*.jpg'))[0]\n",
    "# class_name, confidence, _ = predict_image(model, test_img, class_labels)\n",
    "# print(f\"Image: {test_img.name}\")\n",
    "# print(f\"Pr√©diction: {class_name}\")\n",
    "# print(f\"Confiance: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. R√©sum√©\n",
    "\n",
    "### Fichiers g√©n√©r√©s:\n",
    "- `models/efficientnet_plant_disease.keras` - Mod√®le final\n",
    "- `models/efficientnet_plant_disease.h5` - Format H5\n",
    "- `data/class_labels.json` - Mapping index ‚Üí nom de classe\n",
    "- `models/training_history.png` - Courbes d'entra√Ænement\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "1. Copier le mod√®le dans `C:/TFE-4/models/`\n",
    "2. Cr√©er le fichier `disease_info.json` avec les descriptions des maladies\n",
    "3. Impl√©menter le service de chargement du mod√®le (Epic 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
